<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-5EEPXRT2XL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-5EEPXRT2XL');
  </script>

  <title>Daniel Geng</title>
  
  <meta name="author" content="Daniel Geng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Daniel Geng</name>
              </p>
              <p>
                I am a fifth year PhD student in CS at the University of Michigan advised by Professor <a href="https://andrewowens.com/" target="_blank">Andrew Owens</a>. I study computer vision, and have previously done research in deep reinforcement learning and representation learning. 
              </p>
              <p>
                I spent last summer as a student researcher with <a href="https://scholar.google.com/citations?user=LQvi5XAAAAAJ&hl=en" target="_blank">Charles Herrmann</a> and <a href="https://deqings.github.io/" target="_blank">Deqing Sun</a> at Google DeepMind. In the past I was lucky enough to have worked as an undergrad at UC Berkeley under <a href="https://people.eecs.berkeley.edu/~svlevine/" target="_blank">Sergey Levine</a> and <a href="https://cdevin.github.io/" target="_blank">Coline Devin</a>, as well as <a href="https://people.eecs.berkeley.edu/~efros/" target="_blank">Alyosha Efros</a> and <a href="https://taesung.me/" target="_blank">Taesung Park</a>, and as an intern at FAIR under <a href="https://ltorresa.github.io/home.html" target="_blank">Lorenzo Torresani</a> and <a href="https://csrhddlam.github.io/" target="_blank">Huiyu Wang</a>.
              </p>
              <p style="text-align:center">
                <a href="https://github.com/dangeng/" target="_blank">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=JbhCpzkAAAAJ" target="_blank">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="http://www.linkedin.com/in/dangeng/" target="_blank"> LinkedIn </a> &nbsp/&nbsp -->
                <a href="https://twitter.com/dangengdg" target="_blank"> Twitter </a> &nbsp/&nbsp
                <a href="mailto:dangengdg@gmail.com"> Email </a> <!-- &nbsp/&nbsp -->
                <!--<a href="blog/index.html"> Blog</a>-->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/DanielGeng.v1.png" target="_blank"><img style="width:100%;max-width:100%" alt="profile photo" src="images/DanielGeng.v1.circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am currently interested in generative models, how we can control them, and novel ways to use them. I've also worked on levarging differentiable models of motion for image and video synthesis and understanding, and in the past I've worked on representation learning, multimodal learning, and reinforcement learning.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <video width="160" height="160" autoplay muted loop playsinline>
                  <source src="images/smoke_right.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://motion-prompting.github.io/" target="_blank">
                <papertitle>Motion Prompting: Controlling Video Generation with Motion Trajectories
                </papertitle>
              </a>
              <br>
              <strong>Daniel Geng</strong>, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, Chen Sun, Oliver Wang, Andrew Owens, Deqing Sun
              <br>
              <em>CVPR</em>, 2025 &nbsp;<font color="red"><strong>(Oral)</strong></font> &nbsp
              <br>
              <p></p>
              <p>We train a video generation to be conditioned on motion, and then prompt it with <i>motion prompts</i>, elliciting a wide range of behavior.</p>

              <a href ="https://arxiv.org/abs/2412.02700" target="_blank">arXiv</a> &nbsp/&nbsp
              <a href ="https://motion-prompting.github.io/" target="_blank">webpage</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <video width="160" height="160" autoplay muted loop playsinline>
                  <source src="images/corgi.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ificl.github.io/images-that-sound/" target="_blank">
                <papertitle>Images that Sound: Composing Images and Sounds on a Single Canvas
                </papertitle>
              </a>
              <br>
              Ziyang Chen, <strong>Daniel Geng</strong>, Andrew Owens
              <br>
              <em>NeurIPS</em>, 2024 &nbsp
              <br>
              <p></p>
              <p>We use diffusion models to make spectrograms that look like natural images, but also sound like natural sounds.</p>

              <a href ="https://arxiv.org/abs/2405.12221" target="_blank">arXiv</a> &nbsp/&nbsp
              <a href ="https://ificl.github.io/images-that-sound/" target="_blank">webpage</a> &nbsp/&nbsp
              <a href ="https://github.com/IFICL/images-that-sound" target="_blank">code</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src="images/hybrid.jpg", width=100%>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://dangeng.github.io/factorized_diffusion/" target="_blank">
                <papertitle>Factorized Diffusion: Perceptual Illusions by Noise Decomposition
                </papertitle>
              </a>
              <br>
              <strong>Daniel Geng*</strong>, Inbum Park*, Andrew Owens
              <br>
              <em>ECCV</em>, 2024 &nbsp
              <br>
              <p></p>
              <p>Sister project of "Visual Anagrams." Another zero-shot method for making more types of optical illusions with diffusion models, with connections to spatial and composition control of diffusion models, and inverse problems.</p>

              <a href ="https://arxiv.org/abs/2404.11615" target="_blank">arXiv</a> &nbsp/&nbsp
              <a href ="https://dangeng.github.io/factorized_diffusion/" target="_blank">webpage</a> &nbsp/&nbsp
              <a href ="https://github.com/dangeng/visual_anagrams" target="_blank">code</a>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <video width="160" height="160" autoplay muted loop playsinline>
                  <source src="images/jigsaw.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://dangeng.github.io/visual_anagrams/" target="_blank">
                <papertitle>Visual Anagrams: Synthesizing Multi-View Optical Illusions with Diffusion Models
                </papertitle>
              </a>
              <br>
              <strong>Daniel Geng</strong>, Inbum Park, Andrew Owens
              <br>
              <em>CVPR</em>, 2024 &nbsp;<font color="red"><strong>(Oral)</strong></font> &nbsp
              <br>
              <p></p>
              <p>A simple, zero-shot method to synthesize optical illusions from diffusion models. We introduce <em>Visual Anagrams</em>â€”images that change appearance under a permutation of pixels.</p>

              <a href ="https://arxiv.org/abs/2311.17919" target="_blank">arXiv</a> &nbsp/&nbsp
              <a href ="https://dangeng.github.io/visual_anagrams/" target="_blank">webpage</a> &nbsp/&nbsp
              <a href ="https://github.com/dangeng/visual_anagrams" target="_blank">code</a> &nbsp/&nbsp
              <a href ="https://colab.research.google.com/drive/1hCvJR5GsQrhH1ceDjdbzLG8y6m2UdJ6l?usp=sharing" target="_blank">colab</a>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src="images/motion_guidance.png", width=100%>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://dangeng.github.io/motion_guidance/" target="_blank">
                <papertitle>Motion Guidance: Diffusion-Based Image Editing with Differentiable Motion Estimators
                </papertitle>
              </a>
              <br>
              <strong>Daniel Geng</strong>, Andrew Owens
              <br>
              <em>ICLR</em>, 2024 &nbsp
              <br>
              <p></p>
              <p>We achieve diffusion guidance through off-the-shelf optical flow networks. This enables zero-shot <em>motion based</em> image editing. </p>

              <a href ="https://arxiv.org/abs/2401.18085" target="_blank">arXiv</a> &nbsp/&nbsp
              <a href ="https://dangeng.github.io/motion_guidance/" target="_blank">webpage</a> &nbsp/&nbsp
              <a href ="https://github.com/dangeng/motion_guidance/" target="_blank">code</a>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src="images/motionmag.gif", width=100%>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://dangeng.github.io/flowmag/" target="_blank">
                <papertitle>Self-Supervised Motion Magnification by Backpropagating Through Optical Flow
                </papertitle>
              </a>
              <br>
              <strong>Daniel Geng*</strong>, Zhaoying Pan*, Andrew Owens
              <br>
              <em>NeurIPS</em>, 2023 &nbsp
              <br>
              <p></p>
              <p>By differentiating through off-the-shelf optical flow networks we can train motion magnification models in a fully self-supervised manner.</p>

              <a href ="https://arxiv.org/abs/2311.17056" target="_blank">arXiv</a> &nbsp/&nbsp
              <a href ="https://dangeng.github.io/flowmag/" target="_blank">webpage</a> &nbsp/&nbsp
              <a href ="https://github.com/dangeng/flowmag" target="_blank">code</a>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src="images/comparing_corrs.png", width=100%>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2104.09498" target="_blank">
                <papertitle>Comparing Correspondences: Video Prediction with Correspondence-wise Losses
                </papertitle>
              </a>
              <br>
              <strong>Daniel Geng</strong>, Max Hamilton, Andrew Owens
              <br>
              <em>CVPR</em>, 2022 &nbsp
              <br>
              <p></p>
              <p>Pixelwise losses compare pixels by absolute location. Instead, comparing pixels to their <em>semantic correspondences</em> surprisingly yields better results.</p>

              <a href ="https://arxiv.org/abs/2104.09498" target="_blank">arXiv</a> &nbsp/&nbsp
              <a href ="https://dangeng.github.io/CorrWiseLosses/" target="_blank">webpage</a> &nbsp/&nbsp
              <a href ="https://github.com/dangeng/CorrWiseLosses" target="_blank">code</a>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src="images/smirl.png", width=100%>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1912.05510v4" target="_blank">
                <papertitle>SMiRL: Surprise Minimizing RL in Dynamic Environments</papertitle>
              </a>
              <br>
              Glen Berseth, <strong>Daniel Geng</strong>, Coline Devin, Nicholas Rhinehart, Chelsea Finn, Dinesh Jayaraman, Sergey Levine
              <br>
              <em>ICLR</em>, 2021 &nbsp;<font color="red"><strong>(Oral)</strong></font> &nbsp
              <br>
              <p></p>
              <p>Life seeks order. If we reward an agent for stability do we also get interesting emergent behavior?</p>

              <a href ="https://arxiv.org/abs/1912.05510v4" target="_blank">arXiv</a> &nbsp/&nbsp
              <a href ="https://sites.google.com/view/surpriseminimization" target="_blank">webpage</a> &nbsp/&nbsp
              <a href ="https://iclr.cc/virtual/2021/oral/3453" target="_blank">oral</a>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src="images/cpv2.png", width=100%>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1910.14033" target="_blank">
                <papertitle>Plan Arithmetic: Compositional Plan Vectors for Multi-task Control</papertitle>
              </a>
              <br>
              Coline Devin, <strong>Daniel Geng</strong>, Trevor Darrell, Pieter Abbeel, Sergey Levine
              <br>
              <em>NeurIPS</em>, 2019 &nbsp
              <br>
              <p></p>
              <p>Learning a composable representation of tasks aids in long-horizon generalization of a goal-conditioned policy.</p>

              <a href ="https://arxiv.org/abs/1910.14033" target="_blank">arXiv</a> &nbsp/&nbsp
              <a href ="https://sites.google.com/berkeley.edu/compositionalplanvectors/" target="_blank">webpage</a> &nbsp/&nbsp
              <a href ="https://youtu.be/Z3C_s7bxdyE" target="_blank">short video</a> &nbsp/&nbsp
              <a href ="https://github.com/cdevin/cpv" target="_blank">code</a>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src="images/calibration.png", width=100%>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="assets/calibration.pdf" target="_blank">
                <papertitle>Bayesian Confidence Prediction for Deep Neural Networks</papertitle>
              </a>
              <br>
              Sayna Ebrahimi, <strong>Daniel Geng</strong>, Trevor Darrell
              <br>
              <p></p>
              <p>Given any classification architecture, we can augment it with a confidence network that outputs calibrated class probabilities.</p>
            </td>
          </tr>

          
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right"><font size="2">
                Website template from <a href="http://www.cs.berkeley.edu/~barron/" target="_blank">Jon Barron</a>.
                <br>
                <span id="last-updated">Last updated Apr 2025</span>. <!-- Fallback to Apr 2025 -->
              </font></p>
            </td>
          </tr>
        </table>

      </td>
    </tr>
  </table>

  <!-- Dynamically update "Last updated" line at bottom of page  -->
  <script>
    fetch("https://api.github.com/repos/dangeng/dangeng.github.io/commits?per_page=1")
      .then(response => response.json())
      .then(data => {
        const latestCommitDate = new Date(data[0].commit.committer.date);
        const options = { year: 'numeric', month: 'short' }; // e.g., "Apr 2025"
        document.getElementById("last-updated").textContent =
          "Last updated " + latestCommitDate.toLocaleDateString(undefined, options);
      })
      .catch(error => console.error("Error fetching commit date:", error));
  </script>

</body>

</html>
