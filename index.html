<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-81285507-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-81285507-1');
  </script>

  <title>Daniel Geng</title>
  
  <meta name="author" content="Daniel Geng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Daniel Geng</name>
              </p>
              <p>
                I am a second year PhD student in CS at the University of Michigan advised by Professor <a href="https://andrewowens.com/" target="_blank">Andrew Owens</a>. I currently study computer vision, and in the past I have done research in deep reinforcement learning and representation learning.
              </p>
              </p>
              <p style="text-align:center">
                <a href="https://github.com/dangeng/" target="_blank">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=JbhCpzkAAAAJ" target="_blank">Google Scholar</a> &nbsp/&nbsp
                <a href="http://www.linkedin.com/in/dangeng/" target="_blank"> LinkedIn </a> &nbsp/&nbsp
                <a href="blog/index.html"> Blog</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/DanielGeng.png" target="_blank"><img style="width:100%;max-width:100%" alt="profile photo" src="images/DanielGeng_Circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research currently involves leveraging differetiable physical models of the world for synthesis and understanding of images and videos. In particular I focus on differentiable optical flow networks but am broadly interested in any physical model. I am also interested in representation learning and multimodal learning.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src="images/motion_guidance.png", width=100%>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="#" onclick="return false;">
                <papertitle>Motion Guidance: Zero-Shot Flow-Based Editing with Diffusion Models
                </papertitle>
              </a>
              <br>
              <strong>Daniel Geng*</strong>, Andrew Owens
              <br>
              <em>In submission</em> &nbsp
              <br>
              <p></p>
              <p>We achieve diffusion guidance through off-the-shelf optical flow networks. This enables zero-shot <em>motion based</em> image editing. </p>

              links &nbsp/&nbsp
              coming &nbsp/&nbsp
              soon!
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src="images/motionmag.gif", width=100%>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="#" onclick="return false;">
                <papertitle>Self-Supervised Lagrangian Motion Magnification
                </papertitle>
              </a>
              <br>
              <strong>Daniel Geng*</strong>, Zhaoying Pan*, Andrew Owens
              <br>
              <em>In submission</em> &nbsp
              <br>
              <p></p>
              <p>By differentiating through off-the-shelf optical flow networks we can train motion magnification models in a fully self-supervised manner.</p>

              links &nbsp/&nbsp
              coming &nbsp/&nbsp
              soon!
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src="images/comparing_corrs.png", width=100%>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2104.09498" target="_blank">
                <papertitle>Comparing Correspondences: Video Prediction with Correspondence-wise Losses
                </papertitle>
              </a>
              <br>
              <strong>Daniel Geng</strong>, Max Hamilton, Andrew Owens
              <br>
              <em>CVPR</em>, 2022 &nbsp
              <br>
              <p></p>
              <p>Pixelwise losses compare pixels by absolute location. Instead, comparing pixels to their <em>semantic correspondences</em> surprisingly yields better results.</p>

              <a href ="https://arxiv.org/abs/2104.09498" target="_blank">arxiv</a> &nbsp/&nbsp
              <a href ="https://dangeng.github.io/CorrWiseLosses/" target="_blank">webpage</a> &nbsp/&nbsp
              <a href ="https://github.com/dangeng/CorrWiseLosses"/ target="_blank">code</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src="images/smirl.png", width=100%>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1912.05510v4" target="_blank">
                <papertitle>SMiRL: Surprise Minimizing RL in Dynamic Environments</papertitle>
              </a>
              <br>
              Glen Berseth, <strong>Daniel Geng</strong>, Coline Devin, Nicholas Rhinehart, Chelsea Finn, Dinesh Jayaraman, Sergey Levine
              <br>
              <em>ICLR</em>, 2021 (oral) &nbsp
              <br>
              <p></p>
              <p>Life seeks order. If we reward an agent for stability do we also get interesting emergent behavior?</p>

              <a href ="https://arxiv.org/abs/1912.05510v4" target="_blank">arxiv</a> &nbsp/&nbsp
              <a href ="https://sites.google.com/view/surpriseminimization" target="_blank">webpage</a> &nbsp/&nbsp
              <a href ="https://iclr.cc/virtual/2021/oral/3453" target="_blank">oral</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src="images/cpv2.png", width=100%>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1910.14033" target="_blank">
                <papertitle>Plan Arithmetic: Compositional Plan Vectors for Multi-task Control</papertitle>
              </a>
              <br>
              Coline Devin, <strong>Daniel Geng</strong>, Trevor Darrell, Pieter Abbeel, Sergey Levine
              <br>
              <em>NeurIPS</em>, 2019 &nbsp
              <br>
              <p></p>
              <p>Learning a composable representation of tasks aids in long-horizon generalization of a goal-conditioned policy.</p>

              <a href ="https://arxiv.org/abs/1910.14033" target="_blank">arxiv</a> &nbsp/&nbsp
              <a href ="https://sites.google.com/berkeley.edu/compositionalplanvectors/" target="_blank">webpage</a> &nbsp/&nbsp
              <a href ="https://youtu.be/Z3C_s7bxdyE" target="_blank">short video</a> &nbsp/&nbsp
              <a href ="https://github.com/cdevin/cpv" target="_blank">code</a>

            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src="images/calibration.png", width=100%>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="assets/calibration.pdf" target="_blank">
                <papertitle>Bayesian Confidence Prediction for Deep Neural Networks</papertitle>
              </a>
              <br>
              Sayna Ebrahimi, <strong>Daniel Geng</strong>, Trevor Darrell
              <br>
              <p></p>
              <p>Given any classification architecture, we can augment it with a confidence network that outputs calibrated class probabilities.</p>
            </td>
          </tr>
          
        </tbody></table>

		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
        	<td>
        	  <br>
        	  <p align="right"><font size="2">
          	    Website template from <a href="http://www.cs.berkeley.edu/~barron/" target="_blank">Jon Barron</a>.
	  	      <br>
		        Last updated Jan 2022.
          	  </font>
        	  </p>
        	</td>
      	  </tr>
      	</table>

      </td>
    </tr>
  </table>
</body>

</html>
